# -*- coding: utf-8 -*-
"""Credit Card Default Clients: Classification Model

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1P7H29dqSNjetqKcPgn_I4fNTIzC_li8W

#### DATA INGESTION



*   The dataset default_of_credit_card_clients.csv contains information about credit card holders.
*   We'll inspect the first few rows to get a sense of the features and target.
"""

# Import necessary libraries
from google.colab import files
import pandas as pd
import numpy as np

# Upload the CSV file
#uploaded = files.upload()

# Load the dataset
data = pd.read_csv('default_of_credit_card_clients.csv')

# Set the first row as the column headers
data.columns = data.iloc[0]

# Remove the first row, now that it is the header
data = data[1:].reset_index(drop=True)
dataset = data.copy(deep=True)

# Inspect the first few rows of the data
data.head()

"""#### DATA CLEANING



*   Explore the dataset and check for missing values and duplicate rows as they can distort model training and predictions, so it's essential to handle them, if present.
*   Duplicates, if present can bias the model and lead to overfitting
"""

# Display a summary of the DataFrame
data.info()

# Display the proportion of the target variable
target_proportion = data['default payment next month'].value_counts(normalize=True)
print(target_proportion)

# Check for missing values
print(data.isnull().sum())

# Get the dimensions of the DataFrame
data.shape

# Count the number of duplicated rows in the DataFrame
data.duplicated().sum()

# Display the column labels of the DataFrame
data.columns

# Drop the 'Unnamed: 0' column from the DataFrame
data.drop(['ID'], axis = 1, inplace = True)

# Print the dimensions of the DataFrame after dropping the column
print(data.shape)

# Display the first few rows of the DataFrame to confirm the column has been dropped
data.head()

# Convert continuous variables stored as objects to numerical types
continuous_cols = ['LIMIT_BAL', 'AGE', 'PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6',
                   'BILL_AMT1', 'BILL_AMT2', 'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6',
                   'PAY_AMT1', 'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6']

for col in continuous_cols:
    data[col] = pd.to_numeric(data[col], errors='coerce')  # Convert to numeric, coerce errors to NaN

# Convert the column to int
    data[col] = data[col].astype(int)

# Check for missing values
print(data.isnull().sum())

# Display a summary of the DataFrame
data.info()

"""#### MODEL SELECTION (RANDOM FOREST)

- One-hot encoding enables the model to handle categorical variables by transforming them into a numerical format, ensuring that no ordinal relationships are mistakenly introduced.
*   We're Using random Forest because it is a robust model that can capture complex interactions between features and reduce overfitting
*   Splitting the data ensures we can evaluate how well the model generalizes to unseen data.
*   Random state ensures reproducibility so that each time we run the model, we get the same results.
- Scaling and Data regularization ensures equal contribution of features and prevents overfitting and improved model generalization, helping the overall performance of the model.
"""

from sklearn.preprocessing import OneHotEncoder

# One-Hot Encoding for categorical features
categorical_cols = ['SEX', 'MARRIAGE', 'EDUCATION']
data = pd.get_dummies(data, columns=categorical_cols, drop_first=True)

# Split data into features (X) and target (y)
X = data.drop(columns=['default payment next month'])  # Features
y = data['default payment next month']  # Target

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier

# Splitting data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Data Scaling/Normalization using StandardScaler for continuous columns
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Model Selection (Random Forest Classifier)
model = RandomForestClassifier(n_estimators=100, random_state=42)

"""#### MODEL BUILD

*   Model fitting ensures the model learns the relationships between the features and the target (1 or 0).
*   Predictions: Once trained, the model is tested on unseen data to predict if a customer will default.
"""

# Model Building
model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test)

"""#### MODEL EVALUATION

*   Accuracy tells us the overall percentage of correct predictions.
*   Precision is important when the number of false positives (predicting a default when there isn’t one) is high.
*   Recall is critical when we want to minimize false negatives (missing real defaults)
*   F1 Score is a balance between precision and recall, it is useful when we want to maintain a balance between both metrics.
*   Confusion Matrix breaks down the model's performance in terms of true positives, true negatives, false positives, and false negatives.
- The ROC curve shows the relationship between the True Positive Rate and the False Positive Rate.
"""

from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

# Confusion Matrix
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))

# Classification Report
print("\nClassification Report:")
print(classification_report(y_test, y_pred))

# Evaluate model performance
print("\nAccuracy Score:")
print(accuracy_score(y_test, y_pred))

print(model.predict_proba(X_test)[:, 1])

import matplotlib.pyplot as plt
from sklearn.metrics import roc_auc_score, roc_curve
from sklearn.preprocessing import StandardScaler # Import StandardScaler

# Calculate predicted probabilities for the positive class
y_prob = model.predict_proba(X_test)[:, 1]  # Probabilities for class 1

# Calculate ROC-AUC score
roc_auc = roc_auc_score(y_test.to_list(), y_prob)
print(f'ROC-AUC Score: {roc_auc}')

# Calculate ROC curve
fpr, tpr, thresholds = roc_curve(y_test.to_list(), y_prob, pos_label=1)

# Plot ROC curve
plt.figure()
plt.plot(fpr, tpr, color='darkorange', label=f'ROC curve (area = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', linestyle='--')  # Diagonal line for random guessing
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic')
plt.legend(loc='lower right')
plt.show()

"""#### FINAL POINTS AND CONCLUSIONS

- Final Points and Conclusion
Model Accuracy:
The model achieved an impressive overall accuracy of 81%, indicating that it performs well in making predictions.

- Precision and Recall:

For class 0 (non-defaulters), the precision stood at 0.84, coupled with a high recall of 0.94. This means the model is very effective at spotting non-defaulters, with very few incorrect classifications.
However, for class 1 (defaulters), the precision dropped to 0.65, and the recall was notably lower at 0.37. This suggests that the model struggles to identify many defaulters, leading to a significant number of missed cases.

- F1-Score:
The F1-score tells a story of its own: it’s 0.89 for class 0, indicating strong performance in predicting non-defaulters. In contrast, the F1-score for class 1 is just 0.47, highlighting a clear struggle in accurately predicting defaulters.

- Confusion Matrix:
Looking at the confusion matrix, we see that the model correctly identified 4428 out of 4687 non-defaulters but only 484 out of 1313 defaulters. This imbalance starkly highlights the model’s difficulty in classifying defaulters accurately.

- Macro and Weighted Averages:
The macro average precision and recall sit at 0.75 and 0.66, respectively, indicating that the model's performance is skewed toward class 0. The weighted averages suggest a similar trend, where class 0 dominates the predictions. This imbalance is concerning, especially when identifying defaulters is critical.

- ROC-AUC Score:
The model secured a ROC-AUC score of 0.76, showcasing its ability to distinguish between defaulters and non-defaulters effectively.

Conclusion:
In conclusion, while the model demonstrates commendable accuracy and excels at identifying non-defaulters, there is a pressing need for improvement in its ability to accurately spot defaulters. Addressing these shortcomings will be vital to improving the model's overall performance and ensuring that it reliably identifies high-risk cases.
"""

#Predicting on the whole dataset
X = scaler.fit_transform(X)
dataset.loc[:, 'predicted default payment next month'] = model.predict(X)

#Viewing the new dataframe
dataset.head()

dataset.to_csv('default_of_credit_card_clients_prediction.csv')

from pickle import dump
with open("credit_risk_model.pkl", "wb") as f:
    dump(model, f, protocol=5)

